======== DATA ANALYSIS ========================================

[Anomalous Byte Sent Detection - histogram for 107.3.146.207, Fri, 8 am to 4 pm]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/01/2022:0:0:0 latest=01/01/2023:0:0:0 clientip=107.3.146.207
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip
| eval date = strftime(_time, "%Y/%m/%d") 
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| eval moh = strftime(_time, "%M")
| eval pod = floor(hod / 8) * 8 
| search dow=Fri pod=8
| bin bytes span=100
| stats count by bytes
| sort bytes
| makecontinuous bytes

[Anomalous Byte Sent Detection - histogram for 107.3.146.207, Fri, 8 am to 9 am]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/01/2022:0:0:0 latest=01/01/2023:0:0:0 clientip=107.3.146.207
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip
| eval date = strftime(_time, "%Y/%m/%d") 
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| eval moh = strftime(_time, "%M")
| eval pod = floor(hod / 8) * 8 
| search dow=Fri hod=8
| bin bytes span=100
| stats count by bytes
| sort bytes
| makecontinuous bytes

[Anomalous Byte Sent Detection - histogram for 107.3.146.207, Fri, 8:00 am to 8:05 am]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/01/2022:0:0:0 latest=01/01/2023:0:0:0 clientip=107.3.146.207
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip
| eval date = strftime(_time, "%Y/%m/%d") 
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| eval moh = strftime(_time, "%M")
| eval pod = floor(hod / 8) * 8 
| search dow=Fri hod=8 moh=0
| bin bytes span=100
| stats count by bytes
| sort bytes
| makecontinuous bytes

======== ANOMALY DETECTION BASED ON PROBABILITY ========================================

[Anomalous Byte Sent Detection - create stats model 5m]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/01/2022:0:0:0 latest=01/01/2023:0:0:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip
| eval date = strftime(_time, "%Y/%m/%d") 
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| eval moh = strftime(_time, "%M")
| stats dc(date) as num_date_points_5m count as num_data_points_5m sum(count) as num_raw_points_5m avg(bytes) as avg_bytes_5m stdev(bytes) as stdev_bytes_5m p50(bytes) as p50_bytes_5m p75(bytes) as p75_bytes_5m p25(bytes) as p25_bytes_5m by clientip dow hod moh
| eval iqr_5m = p75_bytes_5m - p25_bytes_5m
| outputlookup bytes_stats_by_clientip_5m.csv

[Anomalous Byte Sent Detection - create stats model 1h]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/01/2022:0:0:0 latest=01/01/2023:0:0:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip
| eval date = strftime(_time, "%Y/%m/%d") 
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| stats dc(date) as num_date_points_1h count as num_data_points_1h sum(count) as num_raw_points_1h avg(bytes) as avg_bytes_1h stdev(bytes) as stdev_bytes_1h p50(bytes) as p50_bytes_1h p75(bytes) as p75_bytes_1h p25(bytes) as p25_bytes_1h by clientip dow hod
| eval iqr_1h = p75_bytes_1h - p25_bytes_1h
| outputlookup bytes_stats_by_clientip_1h.csv

[Anomalous Byte Sent Detection - cluster similar clients for 1 hour span]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/01/2022:0:0:0 latest=01/01/2023:0:0:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip
| eval date = strftime(_time, "%Y/%m/%d") 
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| stats dc(date) as num_date_points count as num_data_points sum(count) as num_raw_points avg(bytes) stdev(bytes) p50(bytes) p75(bytes) p25(bytes) by clientip dow hod
| fit KMeans avg(bytes) stdev(bytes) p50(bytes) p75(bytes) p25(bytes) k=64 as cluster_1h
| eval hod = if(len(hod)<2, "0"+hod, hod)
| outputlookup clientip_clusters_1h.csv

[Anomalous Byte Sent Detection - create probability density model for 1 hour span]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/01/2022:0:0:0 latest=01/01/2023:0:0:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| eval biz = if(date="Sat" OR date="Sun", "off", "on")
| lookup clientip_clusters_1h.csv clientip dow hod output cluster_1h
| fit DensityFunction bytes by "cluster_1h,biz,hod" into app:bytes_probability_density_by_clientip_cluster_1h

[Anomalous Byte Sent Detection - cluster similar clients for 8 hour span]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/01/2022:0:0:0 latest=01/01/2023:0:0:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip 
| eval date = strftime(_time, "%Y/%m/%d") 
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| eval pod = floor(hod / 8) * 8 
| stats dc(date) as num_date_points count as num_data_points sum(count) as num_raw_points avg(bytes) stdev(bytes) p50(bytes) p75(bytes) p25(bytes) by clientip dow pod
| fit KMeans avg(bytes) stdev(bytes) p50(bytes) p75(bytes) p25(bytes) k=64 as cluster_8h
| outputlookup clientip_clusters_8h.csv

[Anomalous Byte Sent Detection - create probability density model for 8 hour span]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/01/2022:0:0:0 latest=01/01/2023:0:0:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| eval pod = floor(hod / 8) * 8 
| eval biz = if(date="Sat" OR date="Sun", "off", "on")
| lookup clientip_clusters_8h.csv clientip dow pod output cluster_8h
| fit DensityFunction bytes by "cluster_8h,biz,pod" into app:bytes_probability_density_by_clientip_cluster_8h

[Anomalous Byte Sent Detection - detect using stats model 5m]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/16/2023:15:25:0 latest=01/16/2023:15:30:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip 
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| eval moh = strftime(_time, "%M")
| lookup bytes_stats_by_clientip_5m.csv clientip dow hod moh
| eval lb_5m = avg_bytes_5m - stdev_bytes_5m * 3, ub_5m = avg_bytes_5m + stdev_bytes_5m * 3
| eval outlier_stats_5m = if(bytes < lb_5m OR bytes > ub_5m, 1, 0)

[Anomalous Byte Sent Detection - detect using stats model 1h]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/16/2023:15:25:0 latest=01/16/2023:15:30:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip 
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| lookup bytes_stats_by_clientip_1h.csv clientip dow hod
| eval lb_1h = avg_bytes_1h - stdev_bytes_1h * 3, ub_1h = avg_bytes_1h + stdev_bytes_1h * 3
| eval outlier_stats_1h = if(bytes < lb_1h OR bytes > ub_1h, 1, 0)

[Anomalous Byte Sent Detection - detect using probability density model for 1 hour span]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/16/2023:15:25:0 latest=01/16/2023:15:30:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| eval biz = if(date="Sat" OR date="Sun", "off", "on")
| lookup clientip_clusters_1h.csv clientip dow hod output cluster_1h
| apply bytes_probability_density_by_clientip_cluster_1h as outlier_pd_1h

[Anomalous Byte Sent Detection - detect using probability density model for 8 hour span]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/16/2023:15:25:0 latest=01/16/2023:15:30:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| eval pod = floor(hod / 8) * 8 
| eval biz = if(date="Sat" OR date="Sun", "off", "on")
| lookup clientip_clusters_8h.csv clientip dow pod output cluster_8h
| apply bytes_probability_density_by_clientip_cluster_8h as outlier_pd_8h

======== ANOMALY DETECTION BASED ON COHESIVE SIMILARITY  ========================================

[Anomalous Byte Sent Detection - detect using cohesive approach for 1 hour span]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/16/2023:15:25:0 latest=01/16/2023:15:30:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| lookup clientip_clusters_1h.csv clientip dow hod output cluster_1h
| eventstats count as count_coh avg(bytes) stdev(bytes) by cluster_1h
| eval lb_coh = 'avg(bytes)' - 3 * 'stdev(bytes)', ub_coh = 'avg(bytes)' + 3 * 'stdev(bytes)'
| eval outlier_coh = if(bytes < lb_coh OR bytes > ub_coh, 1, 0)
| fields _time clientip count bytes cluster_1h lb_coh ub_coh outlier_coh count_coh

======== ANOMALY DETECTION BASED ON VOLUME OF CHANGE  ========================================

[Anomalous Byte Sent Detection - detect using MAD for last 1 hour]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/16/2023:14:25:0 latest=01/16/2023:15:30:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time clientip
| sort clientip _time
| streamstats window=12 median(bytes) as median by clientip reset_on_change=true
| eval abs_dev = abs(bytes - median)
| streamstats window=12 count median(abs_dev) as mad by clientip reset_on_change=true
| eval lb_mad = median - (mad * 1.5), ub_mad = median + (mad * 1.5)
| eval outlier_mad = if(bytes < lb_mad OR bytes > ub_mad, 1, 0)
| fields _time clientip bytes lb_mad ub_mad outlier_mad count mad median
| stats latest(_time) as _time latest(*) as * by clientip
| addinfo
| where _time = (info_max_time - 300)
| fields _time clientip count bytes mad lb_mad ub_mad outlier_mad
| outputlookup bytes_mad_results.csv

======== ANOMALY DETECTION BASED ON PREDICTIONS  ========================================

[Anomalous Byte Sent Detection - detect using prediction results]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/15/2023:15:30:0 latest=01/16/2023:15:30:0
| bin _time span=5m
| stats count sum(bytes) as bytes by _time
| makecontinuous _time
| fillnull
| fit StateSpaceForecast bytes holdback=1h forecast_k=1h conf_interval=99
| addinfo
| where _time = (info_max_time - 300)
| eval outlier_pre = if(bytes < 'lower99(predicted(bytes))' OR bytes > 'upper99(predicted(bytes))', 1, 0)
| fields _time count bytes predicted(bytes) lower99(predicted(bytes)) upper99(predicted(bytes)) outlier_pre

[Anomalous Byte Sent Detection - detect using prediction results (more granular version)]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/9/2023:15:00:0 latest=01/16/2023:16:00:0 
| bin _time span=1h 
| chart sum(bytes) as bytes by _time clientip limit=0
| makecontinuous _time
| fillnull
| predict future_timespan=1 holdback=1 lower99=lower99 upper99=upper99 period=168
    [ search source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/9/2023:15:30:0 latest=01/16/2023:15:30:0 
    | stats count by clientip 
    | stats values(clientip) as clientip 
    | eval clientip = mvjoin(clientip, ",") 
    | return $clientip ]
| tail 1
| untable _time clientip bytes
| rex field=clientip "((?<boundary>(upper|lower)99)\(|)(?<class>prediction)\((?<clientip>[^\)]+)\)"
| eval bytes_actual = if(isnull(class), bytes, null()), bytes_lb = if(match(boundary, "lower"), bytes, null()), bytes_ub = if(match(boundary, "upper"), bytes, null()), bytes_prediction = if(isnull(boundary) AND class="prediction", bytes, null())
| stats values(bytes_*) as bytes_* by _time clientip
| eval outlier_pre = if(bytes < bytes_lb OR bytes > bytes_ub, 1, 0)
| rename bytes_actual as bytes
| fields _time clientip bytes bytes_prediction bytes_lb bytes_ub outlier_pre

======== ANOMALY DETECTION USING COMPOSITE MODELS  ========================================

[Anomalous Byte Sent Detection - detect using composite models]
source="client_bytes.csv.zip:*" host="client_bytes" earliest=01/16/2023:15:25:0 latest=01/16/2023:15:30:0 
| bin _time span=5m 
| stats count sum(bytes) as bytes by _time clientip 
| eval dow = strftime(_time, "%a") 
| eval hod = strftime(_time, "%H") 
| eval moh = strftime(_time, "%M") 
| eval pod = floor(hod / 8) * 8 
| eval biz = if(date="Sat" OR date="Sun", "off", "on") 
| lookup bytes_stats_by_clientip_5m.csv clientip dow hod moh 
| lookup bytes_stats_by_clientip_1h.csv clientip dow hod 
| lookup clientip_clusters_1h.csv clientip dow hod output cluster_1h 
| lookup clientip_clusters_8h.csv clientip dow pod output cluster_8h 
| eventstats count as count_coh avg(bytes) stdev(bytes) by cluster_1h 
| eval lb_5m = avg_bytes_5m - stdev_bytes_5m * 3, ub_5m = avg_bytes_5m + stdev_bytes_5m * 3 
| eval lb_1h = avg_bytes_1h - stdev_bytes_1h * 3, ub_1h = avg_bytes_1h + stdev_bytes_1h * 3 
| eval lb_coh = 'avg(bytes)' - 3 * 'stdev(bytes)', ub_coh = 'avg(bytes)' + 3 * 'stdev(bytes)' 
| eval outlier_stats_5m = if(bytes < lb_5m OR bytes > ub_5m, 1, 0) 
| eval outlier_stats_1h = if(bytes < lb_1h OR bytes > ub_1h, 1, 0) 
| apply bytes_probability_density_by_clientip_cluster_1h as outlier_pd_1h 
| apply bytes_probability_density_by_clientip_cluster_8h as outlier_pd_8h 
| eval outlier_coh = if(bytes < lb_coh OR bytes > ub_coh, 1, 0) 
| fields - BoundaryRanges
| lookup bytes_mad_results.csv _time clientip output mad lb_mad ub_mad outlier_mad
| eval total_outlier_score = 0
| foreach outlier_* 
    [ eval total_outlier_score = if('<<FIELD>>' = 1, total_outlier_score + 1, total_outlier_score) ]
