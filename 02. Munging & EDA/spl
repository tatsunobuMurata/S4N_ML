[Extract fields using rex command]
| tstats count where index=_internal by source
| rex field=source "\/(?<source_file>[^\/]+)$"
| stats sum(count) by source_file

[Convert nested events into a table: 1]
source="nested_event.json" sourcetype="lab_concert"
| spath path=programs{} output=program
| fields program
| fields - _raw
| mvexpand program
| table program

[Convert nested events into a table: 2]
source="nested_event.json" sourcetype="lab_concert"
| spath path=programs{} output=program
| fields program
| fields - _raw
| mvexpand program
| spath input=program path=season
| spath input=program path=orchestra
| spath input=program path=concerts{0}.Date output=date
| spath input=program path=concerts{0}.Time output=time
| spath input=program path=concerts{0}.Venue output=venue
| spath input=program path=concerts{0}.Location output=location
| spath input=program path=works{} output=work
| fields - program
| mvexpand work
| table season orchestra date time venue location work

[Convert nested events into a table: Complete]
source="nested_event.json" sourcetype="lab_concert"
| spath path=programs{} output=program
| fields program
| fields - _raw
| mvexpand program
| spath input=program path=season
| spath input=program path=orchestra
| spath input=program path=concerts{0}.Date output=date
| spath input=program path=concerts{0}.Time output=time
| spath input=program path=concerts{0}.Venue output=venue
| spath input=program path=concerts{0}.Location output=location
| spath input=program path=works{} output=work
| fields - program
| mvexpand work
| spath input=work path=workTitle output=work_title
| spath input=work path=composerName output=composer
| spath input=work path=conductorName output=conductor
| spath input=work path=soloists{}.soloistName output=soloist_names
| spath input=work path=soloists{}.soloistInstrument output=soloiist_instruments
| table season orchestra date time venue location work_title composer conductor soloist_names soloiist_instruments

[Convert nested events into a table: Complete (eval version)]
source="nested_event.json" sourcetype="lab_concert"
| eval program = spath(_raw, "programs{}")
| fields program
| fields - _raw
| mvexpand program
| eval season = spath(program, "season"), orchestra = spath(program, "orchestra"), date = spath(program, "concerts{0}.Date"), time = spath(program, "concerts{0}.Time"), venue = spath(program, "concerts{0}.Venue"), location = spath(program, "concerts{0}.Location")
| eval work = spath(program, "works{}")
| fields - program
| mvexpand work
| eval work_title = spath(work, "workTitle"), composer = spath(work, "composerName"), conductor = spath(work, "conductorName"), soloist_names = spath(work, "soloists{}.soloistName"), soloiist_instruments = spath(work, "soloists{}.soloistInstrument")
| table season orchestra date time venue location work_title composer conductor soloist_names soloiist_instruments

[Convert nested events into a table: outputlookup]
source="nested_event.json" sourcetype="lab_concert"
| eval program = spath(_raw, "programs{}")
| fields program
| fields - _raw
| mvexpand program
| eval season = spath(program, "season"), orchestra = spath(program, "orchestra"), date = spath(program, "concerts{0}.Date"), time = spath(program, "concerts{0}.Time"), venue = spath(program, "concerts{0}.Venue"), location = spath(program, "concerts{0}.Location")
| eval work = spath(program, "works{}")
| fields - program
| mvexpand work
| eval work_title = spath(work, "workTitle"), composer = spath(work, "composerName"), conductor = spath(work, "conductorName"), soloist_names = spath(work, "soloists{}.soloistName"), soloiist_instruments = spath(work, "soloists{}.soloistInstrument")
| table season orchestra date time venue location work_title composer conductor soloist_names soloiist_instruments
| outputlookup lab_concerts.csv

[Convert nested events into a table: inputlookup]
| inputlookup lab_concerts.csv

[Concatenate multiple data sources: _audit]
index=_audit sourcetype=audittrail search_id!="rsa_*" action=search
| rex "search='(?<search_text>[\S\s]+)('(, autojoin|\]\[n\/a\])|'\]$)"
| eval search_text = if(isnull(search_text), search, search_text)
| table search_id total_run_time scan_count event_count search_text

[Concatenate multiple data sources: _introspection]
index=_introspection sourcetype=splunk_resource_usage data.search_props.sid::* component=PerProcess
| table data.search_props.sid data.pct_cpu data.pct_memory

[Concatenate multiple data sources: join]
index=_audit sourcetype=audittrail search_id!="rsa_*" action=search info=completed 
| rex "search='(?<search_text>[\S\s]+)('(, autojoin|\]\[n\/a\])|'\]$)" 
| eval search_text = if(isnull(search_text), search, search_text) 
| rex field=search_id "'(?<search_id>[^']+)" 
| join search_id type=left
    [ search index=_introspection sourcetype=splunk_resource_usage data.search_props.sid::* component=PerProcess 
    | rename data.search_props.sid as search_id ]
| table search_id total_run_time scan_count event_count search_text data.pct_cpu data.pct_memory

[Concatenate multiple data sources: stats]
(index=_audit sourcetype=audittrail search_id!="rsa_*" action=search info=completed) OR
(index=_introspection sourcetype=splunk_resource_usage data.search_props.sid::* component=PerProcess)
| rex "search='(?<search_text>[\S\s]+)('(, autojoin|\]\[n\/a\])|'\]$)" 
| eval search_text = if(isnull(search_text), search, search_text) 
| rex field=search_id "'(?<search_id>[^']+)"
| eval search_id = if(isnull(search_id), 'data.search_props.sid', search_id)
| stats first(total_run_time) first(scan_count) first(event_count) first(search_text) first(data.pct_cpu) first(data.pct_memory) by search_id
| rename first(*) as *

[Convert text data into numeric value: TFDIF fit]
| inputlookup dga_domains 
| fit TFIDF domain analyzer=char ngram_range=2-3 into "dga_ngram" 

[Convert text data into numeric value: TFDIF apply]
| inputlookup dga_domains.csv where class=legit
| head 5
| append 
    [ inputlookup dga_domains.csv where class=dga
    | head 5 ] 
| apply dga_ngram

[Convert text data into numeric value: URL Toolbox]
| inputlookup dga_domains.csv where class=legit
| head 5
| append 
    [ inputlookup dga_domains.csv where class=dga
    | head 5 ] 
| lookup ut_shannon_lookup word as domain
| lookup ut_meaning_lookup word as domain

[Convert text data into numeric value: Other techniques]
| inputlookup dga_domains.csv where class=legit
| head 5
| append 
    [ inputlookup dga_domains.csv where class=dga
    | head 5 ] 
| eval ut_digit_ratio=0.0, ut_vowel_ratio=0.0, ut_domain_length=max(1,len(domain))
| rex field=domain max_match=0 "(?<digits>\\d)"
| rex field=domain max_match=0 "(?<vowels>[aeiou])"
| eval ut_digit_ratio=if(isnull(digits),0.0,(mvcount(digits) / ut_domain_length)), ut_vowel_ratio=if(isnull(vowels),0.0,(mvcount(vowels) / ut_domain_length)), ut_consonant_ratio=max(0.0,((1.000000 - ut_digit_ratio) - ut_vowel_ratio))
| fields - digits, "-", vowels 

[Understand the shape of data: Histogram (original data)]
| inputlookup logins.csv

[Understand the shape of data: Histogram]
| inputlookup logins.csv
| eval day_of_week = strftime(_time, "%a")
| eval hour_of_day = strftime(_time, "%H")
| search day_of_week IN (Mon, Tue, Wed, Thu, Fri) hour_of_day IN (0, 1, 2, 3, 4, 5, 6)
| bin logins span=10
| stats count by logins
| makecontinuous logins

[Understand the shape of data: Time Series Data]
| inputlookup iron.csv
| fields month iron_production

[Understand the shape of data: Seasonality: check]
| inputlookup iron.csv
| fields month iron_production
| autoregress iron_production
| eval diff = iron_production - iron_production_p1
| fillnull
| fit ACF diff

[Understand the shape of data: Seasonality: extraction]
| inputlookup iron.csv
| fields month iron_production
| x11 add12(iron_production) as deseason
| eval seasonality = iron_production - deseason
| fields month seasonality

[Understand the shape of data: Trend]
| inputlookup iron.csv
| fields month iron_production
| trendline sma12(iron_production)

[Understand the shape of data: Residuals]
| inputlookup iron.csv
| fields month iron_production
| trendline sma12(iron_production) as trend
| x11 add12(iron_production) as deseason
| eval residual = deseason - trend
| fields month residual

[Find feature variables: Pearson correlation coefficient]
| inputlookup server_power.csv
| score pearsonr ac_power against total-cpu-utilization

[Find feature variables: Spearman rank correlation coefficient]
| inputlookup server_power.csv
| score spearmanr ac_power against total-cpu-utilization

[Find feature variables: analyzefield]
| inputlookup firewall_traffic.csv 
| head 50000 
| analyzefields classfield=used_by_malware

[Find feature variables: associate]
| inputlookup disk_failures.csv
| associate

[Find feature variables: FieldSelector]
| inputlookup server_power.csv 
| fit RobustScaler total-*
| fit FieldSelector ac_power from RS_*
| fields _time ac_power fs_*

[Visualization]
| inputlookup dga_domains.csv where class=legit 
| head 5 
| append 
    [ inputlookup dga_domains.csv where class=dga 
    | head 5 ] 
| apply dga_ngram 
| apply dga_pca 
| fields class PC_*
| fields - _time

[Outlier handling in training data: null hypothesis test for normal distribution]
| inputlookup server_power.csv 
| score normaltest total-disk-blocks

[Outlier handling in training data: anomalydetection action=filter (default)]
| inputlookup server_power.csv 
| anomalydetection

[Outlier handling in training data: anomalydetection action=annotate]
| inputlookup server_power.csv 
| anomalydetection action=annotate
| where isnull(probable_cause)

[Outlier handling in training data: histogram after removing outliers method=histogram]
| inputlookup server_power.csv 
| anomalydetection action=annotate
| where isnull(probable_cause)
| bin total-disk-blocks span=10
| stats count by total-disk-blocks
| makecontinuous total-disk-blocks

[Outlier handling in training data: histogram after removing outliers method=zscore]
| inputlookup server_power.csv 
| anomalydetection method=zscore action=annotate 
| where isnull('Anomaly_Score_Num(total-disk-blocks)') 
| bin total-disk-blocks span=10 
| stats count by total-disk-blocks 
| makecontinuous total-disk-blocks

[Outlier handling in training data: histogram after removing outliers method=iqr]
| inputlookup server_power.csv 
| anomalydetection method=iqr action=rm
| bin total-disk-blocks span=10
| stats count by total-disk-blocks
| makecontinuous total-disk-blocks
